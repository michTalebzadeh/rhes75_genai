class VAE(Model):
  """
  This class defines a Variational Autoencoder (VAE) architecture.

  A VAE is a neural network architecture that learns a latent representation
  of data. It consists of an encoder and a decoder network. The encoder
  compresses the input data into a lower-dimensional latent space, while the
  decoder attempts to reconstruct the original data from the latent representation.

  This specific implementation uses a dense neural network architecture for
  both the encoder and decoder.
  """

  def __init__(self, input_dim, latent_dim):
    """
    Initializes the VAE model.

    Args:
      input_dim: The dimensionality of the input data.
      latent_dim: The dimensionality of the latent space.
    """
    super(VAE, self).__init__()  # Call the Model class constructor
    self.input_dim = input_dim
    self.latent_dim = latent_dim
    self.encoder = self.build_encoder()  # Create the encoder network
    self.decoder = self.build_decoder()  # Create the decoder network

  def build_encoder(self):
    """
    Defines the encoder network architecture.

    The encoder takes the input data and compresses it into two outputs:
      - z_mean: The mean of the latent distribution.
      - z_log_var: The log variance of the latent distribution.

    Returns:
      A Keras Model representing the encoder network.
    """
    inputs = layers.Input(shape=(self.input_dim,))
    x = layers.Dense(256, activation='relu')(inputs)  # First hidden layer with ReLU activation
    z_mean = layers.Dense(self.latent_dim)(x)  # Layer for the mean of latent space
    z_log_var = layers.Dense(self.latent_dim)(x)  # Layer for the log variance of latent space
    return Model(inputs, [z_mean, z_log_var])  # Return a model with these two outputs

  def build_decoder(self):
    """
    Defines the decoder network architecture.

    The decoder takes the latent representation (z) and attempts to reconstruct
    the original input data.

    Returns:
      A Keras Model representing the decoder network.
    """
    latent_inputs = layers.Input(shape=(self.latent_dim,))
    x = layers.Dense(256, activation='relu')(latent_inputs)  # First hidden layer with ReLU activation
    outputs = layers.Dense(self.input_dim, activation='sigmoid')(x)  # Output layer with sigmoid activation
    return Model(latent_inputs, outputs)  # Return a model with the reconstructed data as output

  def sample(self, z_mean, z_log_var):
    """
    Samples a point from the latent distribution defined by z_mean and z_log_var.

    This is used during training to generate random points in the latent space
    for decoder training.

    Args:
      z_mean: The mean of the latent distribution.
      z_log_var: The log variance of the latent distribution.

    Returns:
      A sampled point from the latent distribution.
    """
    epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], self.latent_dim))  # Sample random noise
    return z_mean + tf.exp(0.5 * z_log_var) * epsilon  # Reparameterization trick

  def call(self, inputs):
    """
    Defines the forward pass of the VAE model.

    This method combines the encoder and decoder to perform the VAE operation.

    Args:
      inputs: The input data to be encoded and reconstructed.

    Returns:
      The reconstructed data from the decoder.
    """
    z_mean, z_log_var = self.encoder(inputs)  # Encode the input data
    z = self.sample(z_mean, z_log_var)  # Sample a point from the latent space
    return self.decoder(z)  # Decode the latent representation
